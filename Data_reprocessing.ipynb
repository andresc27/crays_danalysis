{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982fc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import bz2, os, time\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdate\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c1f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS    = 32  # 32 bines por pulso\n",
    "ADCBITS  = 14  # 14 bits\n",
    "TIME_SEP = 8e-9   # 8 ns por punto\n",
    "BASELINE = 0   # linea de base\n",
    "channels = [1,2]\n",
    "\n",
    "data_dir = '/eos/user/d/dcazarra/datos_panchito/' # Directorio donde se encuentran los datos\n",
    "#data_dir = '/eos/home-i00/d/dmerizal/SWAN_projects/LAGO/Datos/'\n",
    "#data_dir = '/eos/user/d/dmerizal/SWAN_projects/LAGO/Datos/' \n",
    "plot_dir = 'plots' # Directorio donde se guardaran las graficas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae1dbd",
   "metadata": {},
   "source": [
    "## Extracting and saving data as .csv files to reduce the space in disk that original .dat files occupy. Then we can have enough memory to analyse data in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f57235fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing data from 2024_05_01...\n",
      "           tcounter  counts      tdiff\n",
      "0            572656     0.0        0.0\n",
      "1           1404534     1.0   831878.0\n",
      "2           1509143     1.0   104609.0\n",
      "3           1808749     1.0   299606.0\n",
      "4           1860276     1.0    51527.0\n",
      "...             ...     ...        ...\n",
      "30136080  121962205     1.0    40009.0\n",
      "30136081  122475722     1.0   513517.0\n",
      "30136082  122818120     1.0   342398.0\n",
      "30136083  123469879     1.0   651759.0\n",
      "30136084  124826557     1.0  1356678.0\n",
      "\n",
      "[30136085 rows x 3 columns]\n",
      "wrap around points 86405\n",
      "Length is 86400\n",
      "Saved as raw_data_2024_05_01.csv\n"
     ]
    }
   ],
   "source": [
    "#dates and hours of the corresponding datasets \n",
    "dates = ['2024_05_01','2024_05_02', '2024_05_03', '2024_05_04', '2024_05_05','2024_05_06', '2024_05_07', '2024_05_08', '2024_05_09', '2024_05_10', '2024_05_11', '2024_05_12', '2024_05_13']\n",
    "hours = ['00','01','02','03', '04','05','06', '07','08','09','10','11','12','13','14','15','16', '17', '18','19','20','21','22','23']\n",
    "\n",
    "# Initialize lists to store the data\n",
    "for date in dates:\n",
    "    temperature = []\n",
    "    pressure = []\n",
    "    counts = []\n",
    "    t_counter = []\n",
    "    print(f'Analysing data from {date}...')\n",
    "    # Iterate through each hour and read the data\n",
    "    for hour in hours:\n",
    "        filename = f't600_nogps_{date}_{hour}h00.dat'\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File {filename} not found.\")\n",
    "            continue\n",
    "\n",
    "        # Load data from the file\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    # Check for temperature lines\n",
    "                    if line.startswith('# x t'):\n",
    "                        temperature.append(float(line.strip().split()[3]))\n",
    "                    elif line.startswith('# x p'):\n",
    "                    # Check for pressure lines\n",
    "                        pressure.append(float(line.strip().split()[3]))\n",
    "                    # Check for counter (internal trigger clock)\n",
    "                    elif line.startswith('# t'):\n",
    "                        t_counter.append(int(line.strip().split()[3]))\n",
    "                    # Check for counter lines (detection counter)\n",
    "                    elif line.startswith('# c'):\n",
    "                        counts.append(int(line.strip().split()[2]))\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "    \n",
    "    df_trigger = pd.DataFrame({\n",
    "        'tcounter': t_counter, #number of trigger cyles\n",
    "        'counts': counts\n",
    "    })\n",
    "\n",
    "    ## Display the DataFrame\n",
    "    df_trigger['counts'] = df_trigger['counts'].diff().fillna(0)\n",
    "    df_trigger['tdiff'] = df_trigger['tcounter'].diff().fillna(0)\n",
    "\n",
    "    print(df_trigger)\n",
    "    \n",
    "    \n",
    "    # Define the clock frequency and maximum tcycles per second\n",
    "    clock_frequency = 125e6  # 125 MHz\n",
    "    tcycles_per_second = clock_frequency\n",
    "\n",
    "    # Identify wrap-around points (points where the clock counter is restarted)\n",
    "    wrap_around_points = df_trigger['tdiff']  < 0\n",
    "\n",
    "    count_trues = sum(wrap_around_points)\n",
    "    print(f'wrap around points {count_trues}')  # Output should be 3600*N-1  (N is the number of hours)\n",
    "\n",
    "    # Adjust for wrap-around points\n",
    "    cumulative_wrap_around = wrap_around_points.cumsum()\n",
    "\n",
    "    df_trigger['tcycles_adj'] = df_trigger['tdiff'] + cumulative_wrap_around * tcycles_per_second\n",
    "\n",
    "    # Calculate time in seconds\n",
    "    df_trigger['time'] = np.floor(df_trigger['tcycles_adj'] / tcycles_per_second).astype(int)+1\n",
    "\n",
    "\n",
    "    # Group the 'counts' based on 'time'\n",
    "    grouped = df_trigger.groupby('time')['counts'].apply(list)\n",
    "\n",
    "    # Convert the result to a list of lists (subarrays)\n",
    "    results = np.array(grouped.tolist(), dtype=object)\n",
    "    del df_trigger\n",
    "    # Each subarray has the counts on each second. In this case, there should be roughly more than 230 hits in each one (mean value is 350 per second)\n",
    "    # If less than 230 it could mean the electronics made a mistake, therefore that second will not be taken into account \n",
    "    filtered_results = [sum(subarray ) for subarray in results if sum(subarray) > 230 ]\n",
    "\n",
    "    # Convert the filtered result to a NumPy array with dtype=object\n",
    "    counts = np.array(filtered_results, dtype=float).flatten()\n",
    "    pressure = np.array(pressure, dtype=float).flatten()\n",
    "    temperature = np.array(temperature, dtype=float).flatten()\n",
    "    \n",
    "    # Making sure arrays have same lenght \n",
    "    # It is common the counts to have more elements than pressure, e.g. one hour of data taking should have 3600 measurements of pressure\n",
    "    min_length = min(len(counts), len(pressure))\n",
    "    counts = counts[:min_length]\n",
    "    print(f'Length is {min_length}')\n",
    "     \n",
    "    df_raw = pd.DataFrame({\n",
    "        'counts': counts, #number of detections\n",
    "        'pressure': pressure,\n",
    "        'temperature': temperature\n",
    "    })\n",
    "    ### save as a .csv \n",
    "    df_raw.to_csv(f'raw_data_{date}.csv', index=False)\n",
    "    print(f'Saved as raw_data_{date}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc0e3ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         counts  pressure  temperature\n",
      "0         390.0    766.82         22.9\n",
      "1         324.0    766.73         22.9\n",
      "2         363.0    766.76         23.0\n",
      "3         330.0    766.79         22.9\n",
      "4         365.0    766.82         22.9\n",
      "...         ...       ...          ...\n",
      "1123195   379.0    766.92         22.5\n",
      "1123196   326.0    767.05         22.5\n",
      "1123197   377.0    767.01         22.5\n",
      "1123198   368.0    766.94         22.5\n",
      "1123199   354.0    766.94         22.5\n",
      "\n",
      "[1123200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Loading and check reprocessed datasets\n",
    "# List of dates corresponding to CSV filenames\n",
    "dates = ['2024_05_01', '2024_05_02','2024_05_03', '2024_05_04','2024_05_05', '2024_05_06', '2024_05_07', '2024_05_08', '2024_05_09', '2024_05_10', '2024_05_11', '2024_05_12', '2024_05_13']\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the dates and read each CSV file\n",
    "for date in dates:\n",
    "    filename = f'raw_data_{date}.csv'  # Construct the filename dynamically\n",
    "    df = pd.read_csv(filename)  # Read the CSV file\n",
    "    dataframes.append(df)       # Append the DataFrame to the list\n",
    "\n",
    "# Concatenate all the DataFrames into one\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac895b33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
